{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9734147,"sourceType":"datasetVersion","datasetId":5957319}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ§‘â€ðŸ« Task 1 Part 2: Build Your Own Logistic Regression Model for Sentiment Analysis\nIn this exercise, you will build a **logistic regression model** from scratch to perform sentiment analysis.\n\n**Objective:** Implement all key components of an ML pipeline (except for data handling).\n\n**Allowed Libraries:** `pandas`, `numpy`\n\n**Not Allowed:** Any pre-built ML algorithms or functions like `LogisticRegression` from `sklearn`.\n\nFollow the instructions step-by-step and answer the questions!","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:04:43.518812Z","iopub.execute_input":"2024-10-27T15:04:43.519277Z","iopub.status.idle":"2024-10-27T15:04:44.675552Z","shell.execute_reply.started":"2024-10-27T15:04:43.519224Z","shell.execute_reply":"2024-10-27T15:04:44.674367Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/ml-trek-imdb/IMDB_Dataset.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Step 1: Load the Data\n**Task:** Use `pandas` to load the dataset from a file named `IMDB_reviews.csv`.\n\n> **Hint:** Use `pd.read_csv()` to load the file and display the first 5 rows.\n\n**Question:** What are the key features and the target variable in this dataset?","metadata":{}},{"cell_type":"markdown","source":"The key features and the review and the target is te sentiment","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/ml-trek-imdb/IMDB_Dataset.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:04:47.454628Z","iopub.execute_input":"2024-10-27T15:04:47.455177Z","iopub.status.idle":"2024-10-27T15:04:48.898051Z","shell.execute_reply.started":"2024-10-27T15:04:47.455135Z","shell.execute_reply":"2024-10-27T15:04:48.896979Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:04:48.900106Z","iopub.execute_input":"2024-10-27T15:04:48.900687Z","iopub.status.idle":"2024-10-27T15:04:48.920642Z","shell.execute_reply.started":"2024-10-27T15:04:48.900633Z","shell.execute_reply":"2024-10-27T15:04:48.919470Z"},"trusted":true},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:04:51.078174Z","iopub.execute_input":"2024-10-27T15:04:51.078885Z","iopub.status.idle":"2024-10-27T15:04:51.127167Z","shell.execute_reply.started":"2024-10-27T15:04:51.078814Z","shell.execute_reply":"2024-10-27T15:04:51.125988Z"},"trusted":true},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 50000 entries, 0 to 49999\nData columns (total 2 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   review     50000 non-null  object\n 1   sentiment  50000 non-null  object\ndtypes: object(2)\nmemory usage: 781.4+ KB\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Step 3: Tokenization and Text Cleaning\n**Task:** Implement your own function to:\n1. Convert all text to lowercase.\n2. Remove punctuation and special characters.\n3. Split the text into words (tokenization).\n\n> **Hint:** Use Python string methods and list comprehensions.\n\n**Question:** Why is tokenization important for text-based models?","metadata":{}},{"cell_type":"code","source":"import re\n\ndef tokenize_clean(text):\n    text = text.lower()\n    text = re.sub(r'[^a-z\\s]', '', text)\n    tokens = text.split()\n    \n    return tokens\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:04:54.166266Z","iopub.execute_input":"2024-10-27T15:04:54.166726Z","iopub.status.idle":"2024-10-27T15:04:54.172948Z","shell.execute_reply.started":"2024-10-27T15:04:54.166684Z","shell.execute_reply":"2024-10-27T15:04:54.171781Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"tokenize_clean(data['review'][0])[0:10]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:01:28.718515Z","iopub.execute_input":"2024-10-27T15:01:28.718919Z","iopub.status.idle":"2024-10-27T15:01:28.727268Z","shell.execute_reply.started":"2024-10-27T15:01:28.718882Z","shell.execute_reply":"2024-10-27T15:01:28.726037Z"},"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"['one',\n 'of',\n 'the',\n 'other',\n 'reviewers',\n 'has',\n 'mentioned',\n 'that',\n 'after',\n 'watching']"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"transforms raw text into individual units, typically words, that can be processed by the model. \ntokenization helps in creating a structured, consistent input for the model, making it possible to analyze word frequency, context, and sentiment patterns effectively.","metadata":{}},{"cell_type":"markdown","source":"## Step 4: Create a Vocabulary\n**Task:** Create a **vocabulary** (a list of unique words) from the tokenized dataset.\n\n> **Hint:** Use a set to store unique words, then convert it to a list.\n\n**Question:** How does vocabulary size affect model performance?","metadata":{}},{"cell_type":"code","source":"vocabulary = set()\nfor review in data['review']:\n    tokens = tokenize_clean(review)\n    vocabulary.update(tokens)\n    \nvocabulary = sorted(list(vocabulary))","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:05:02.230513Z","iopub.execute_input":"2024-10-27T15:05:02.230964Z","iopub.status.idle":"2024-10-27T15:05:07.422246Z","shell.execute_reply.started":"2024-10-27T15:05:02.230919Z","shell.execute_reply":"2024-10-27T15:05:07.420912Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"vocabulary[:25], len(vocabulary)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:05:07.424647Z","iopub.execute_input":"2024-10-27T15:05:07.425418Z","iopub.status.idle":"2024-10-27T15:05:07.435235Z","shell.execute_reply.started":"2024-10-27T15:05:07.425358Z","shell.execute_reply":"2024-10-27T15:05:07.433889Z"},"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(['a',\n  'aa',\n  'aaa',\n  'aaaaaaaaaaaahhhhhhhhhhhhhh',\n  'aaaaaaaargh',\n  'aaaaaaah',\n  'aaaaaaahhhhhhggg',\n  'aaaaagh',\n  'aaaaah',\n  'aaaaargh',\n  'aaaaarrrrrrgggggghhhhhh',\n  'aaaaatchkah',\n  'aaaaaw',\n  'aaaahhhhhh',\n  'aaaahhhhhhh',\n  'aaaand',\n  'aaaarrgh',\n  'aaaawwwwww',\n  'aaaggghhhhhhh',\n  'aaagh',\n  'aaah',\n  'aaahhhhhhh',\n  'aaahthe',\n  'aaall',\n  'aaand'],\n 175891)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"limited_vocab_index = {word: i for i, word in enumerate(vocabulary)}","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:35:25.415761Z","iopub.execute_input":"2024-10-27T15:35:25.416315Z","iopub.status.idle":"2024-10-27T15:35:25.520512Z","shell.execute_reply.started":"2024-10-27T15:35:25.416270Z","shell.execute_reply":"2024-10-27T15:35:25.519387Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"Larger Vocabulary: It captures more details and nuances in language, potentially improving accuracy by allowing the model to learn from more unique words\na large vocabulary also increases computational requirements\nmight include many low-frequency or irrelevant words.","metadata":{}},{"cell_type":"markdown","source":"## Step 5: Implement Word Count\n**Task:** Calculate and store the number of times each word appears in a particular review for all reviews","metadata":{}},{"cell_type":"code","source":"word_count_dict = {}\n\nfor review in data['review']:\n    tokens = tokenize_clean(review)\n    for word in tokens:\n        if word in word_count_dict:\n            word_count_dict[word] += 1\n        else:\n            word_count_dict[word] = 1\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:27:18.884026Z","iopub.execute_input":"2024-10-27T15:27:18.885289Z","iopub.status.idle":"2024-10-27T15:27:28.385426Z","shell.execute_reply.started":"2024-10-27T15:27:18.885220Z","shell.execute_reply":"2024-10-27T15:27:28.384353Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"top_10_words = sorted(word_count_dict.items(), key=lambda item: item[1], reverse=True)[:10]\ntop_10_words","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 6: Train-Test Split\n**Task:** Split the data into **80% training** and **20% testing** sets.\n\n> **Hint:** Use `numpy` or list slicing to split the data manually.\n\n**Question:** Why do we need to split the data for training and testing?","metadata":{}},{"cell_type":"code","source":"np.random.seed(42)\n\nshuffled_indices = np.random.permutation(len(data))\n\nsplit_index = int(0.8 * len(data))\n\ntrain_indices = shuffled_indices[:split_index]\ntest_indices = shuffled_indices[split_index:]","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:32:51.008033Z","iopub.execute_input":"2024-10-27T15:32:51.008462Z","iopub.status.idle":"2024-10-27T15:32:51.017764Z","shell.execute_reply.started":"2024-10-27T15:32:51.008420Z","shell.execute_reply":"2024-10-27T15:32:51.016323Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"code","source":"y_train = np.array([1 if label == 'positive' else 0 for label in train_data['sentiment']])","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:32:53.424075Z","iopub.execute_input":"2024-10-27T15:32:53.424535Z","iopub.status.idle":"2024-10-27T15:32:53.446356Z","shell.execute_reply.started":"2024-10-27T15:32:53.424483Z","shell.execute_reply":"2024-10-27T15:32:53.445067Z"},"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":"X_train = np.zeros((len(train_data), len(limited_vocab_index)))","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:39:01.200308Z","iopub.execute_input":"2024-10-27T15:39:01.201317Z","iopub.status.idle":"2024-10-27T15:39:01.207275Z","shell.execute_reply.started":"2024-10-27T15:39:01.201263Z","shell.execute_reply":"2024-10-27T15:39:01.206022Z"},"trusted":true},"outputs":[],"execution_count":30},{"cell_type":"code","source":"for i, review in enumerate(train_data['review']):\n    tokens = tokenize_clean(review)\n    for word in tokens:\n        if word in limited_vocab_index:\n            X_train[i, limited_vocab_index[word]] += 1","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:50:23.676163Z","iopub.execute_input":"2024-10-27T15:50:23.677199Z","iopub.status.idle":"2024-10-27T15:51:21.252462Z","shell.execute_reply.started":"2024-10-27T15:50:23.677147Z","shell.execute_reply":"2024-10-27T15:51:21.251312Z"},"trusted":true},"outputs":[],"execution_count":38},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:51:45.541102Z","iopub.execute_input":"2024-10-27T15:51:45.542243Z","iopub.status.idle":"2024-10-27T15:51:45.551385Z","shell.execute_reply.started":"2024-10-27T15:51:45.542179Z","shell.execute_reply":"2024-10-27T15:51:45.549944Z"},"trusted":true},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"array([[5., 0., 0., ..., 0., 0., 0.],\n       [5., 0., 0., ..., 0., 0., 0.],\n       [3., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [8., 0., 0., ..., 0., 0., 0.],\n       [3., 0., 0., ..., 0., 0., 0.],\n       [6., 0., 0., ..., 0., 0., 0.]])"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"train_data = data.iloc[train_indices]\ntest_data = data.iloc[test_indices]\n\nprint(\"Training set size:\", train_data.shape)\nprint(\"Testing set size:\", test_data.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:51:54.134075Z","iopub.execute_input":"2024-10-27T15:51:54.134557Z","iopub.status.idle":"2024-10-27T15:51:54.149656Z","shell.execute_reply.started":"2024-10-27T15:51:54.134511Z","shell.execute_reply":"2024-10-27T15:51:54.148400Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Training set size: (40000, 2)\nTesting set size: (10000, 2)\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"## Step 7: Building the Logistic Regression Model (Divided Steps)","metadata":{}},{"cell_type":"markdown","source":"### Part 1: The Prediction functions\nThe **prediction function** returns the predicted value of the data point using the weights and the bias. It uses the sigmoid function to convert the prediction into a value in the range of 0 to 1.\n\n**Task:** Implement the sigmoid and prediction functions","metadata":{}},{"cell_type":"code","source":"def sigmoid(x):\n return 1 / (1 + np.exp(-x))\n\ndef lr_prediction(weights,\tbias,\tfeatures):\n    z = np.dot(features, weights) + bias\n    return sigmoid(z)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:51:58.468004Z","iopub.execute_input":"2024-10-27T15:51:58.468435Z","iopub.status.idle":"2024-10-27T15:51:58.474601Z","shell.execute_reply.started":"2024-10-27T15:51:58.468394Z","shell.execute_reply":"2024-10-27T15:51:58.473403Z"},"trusted":true},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"### Part 2: Implementing the Error functions\n**Task:** Use the gradient update rules to train the logistic regression model over multiple epochs.","metadata":{}},{"cell_type":"code","source":"def\tlog_loss(weights,\tbias,\tfeatures,\tlabel):\n    prediction = lr_prediction(weights, bias, features)\n    return - (label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n\ndef\ttotal_log_loss(weights,\tbias,\tX,\ty):\n    losses = [log_loss(weights, bias, X[i], y[i]) for i in range(len(y))]\n    return np.mean(losses)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:52:00.820509Z","iopub.execute_input":"2024-10-27T15:52:00.820934Z","iopub.status.idle":"2024-10-27T15:52:00.828717Z","shell.execute_reply.started":"2024-10-27T15:52:00.820894Z","shell.execute_reply":"2024-10-27T15:52:00.827545Z"},"trusted":true},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"### Part 1: Update Weights\nThe **Update_Weights** adjusts weights and bias based on whether points are correctly or incorrectly classified, It is a simple method of improving the model at every iteration:\n1. **Correctly classified points:** Move the line **away** from the point.\n2. **Incorrectly classified points:** Move the line **towards** the point.\n\n**Task:** Implement the gradient update function based on these rules.","metadata":{}},{"cell_type":"code","source":"def\tupdate_weights(weights,\tbias,\tfeatures,\tlabel,\tlearning_rate\t=\t0.01):\n    prediction = lr_prediction(weights, bias, features)\n    error = prediction - label\n    for i in range(len(weights)):\n        weights[i] -= learning_rate * error * features[i]\n        bias -= learning_rate * error\n    return weights, bias","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:52:02.926793Z","iopub.execute_input":"2024-10-27T15:52:02.927249Z","iopub.status.idle":"2024-10-27T15:52:02.934469Z","shell.execute_reply.started":"2024-10-27T15:52:02.927204Z","shell.execute_reply":"2024-10-27T15:52:02.933177Z"},"trusted":true},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"### Part 2: Implementing the Logistic Regression Algorithm\n**Task:** Use the function to update weights to train the logistic regression model over multiple epochs. Keep track of the total error for each epoch. You will later plot these errors.","metadata":{}},{"cell_type":"code","source":"def lr_algorithm(features, labels, learning_rate=0.01, epochs=200):\n    weights = np.zeros(features.shape[1])\n    bias = 0\n    error_history = []\n\n    for epoch in range(epochs):\n        for i in range(len(labels)):\n            weights, bias = update_weights(weights, bias, features[i], labels[i], learning_rate)\n        \n        total_error = total_log_loss(weights, bias, features, labels)\n        error_history.append(total_error)\n    \n    return weights, bias, error_historyy","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:52:04.797513Z","iopub.execute_input":"2024-10-27T15:52:04.797972Z","iopub.status.idle":"2024-10-27T15:52:04.806104Z","shell.execute_reply.started":"2024-10-27T15:52:04.797927Z","shell.execute_reply":"2024-10-27T15:52:04.804989Z"},"trusted":true},"outputs":[],"execution_count":44},{"cell_type":"code","source":"weights, bias, error_history = lr_algorithm(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T15:52:07.605473Z","iopub.execute_input":"2024-10-27T15:52:07.605932Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_90/653254292.py:2: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-x))\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Step 8: Evaluate Your Model\n**Task:** Calculate the accuracy of the model. Compare the predicted labels with the actual labels.\n\n> **Hint:** Use the formula for accuracy: (Correct Predictions / Total Predictions) * 100\n\n**Question:** Which metricâ€”accuracy, precision, or recallâ€”is most important for sentiment analysis?","metadata":{}},{"cell_type":"code","source":"def calculate_accuracy(weights, bias, features, labels):\n    predictions = [1 if lr_prediction(weights, bias, features[i]) >= 0.5 else 0 for i in range(len(labels))]\n    correct_predictions = sum([1 for i in range(len(labels)) if predictions[i] == labels[i]])\n    accuracy = (correct_predictions / len(labels)) * 100\n    return accuracy\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"sentiment analysis, precision is more important, especially in cases where false positives (incorrectly classifying negative sentiment as positive) are more impactful.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 8: Visualize the Errors  \n**Task:** Create a scatter plot of the total errors over the training epochs. The plot should show a gradual decrease in errors, stabilizing as the model converges.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_errors(error_history):\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(len(error_history)), error_history, marker='o', linestyle='-', color='b')\n    plt.title(\"Total Log Loss Over Training Epochs\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Log Loss\")\n    plt.show()\n\nplot_errors(error_history)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 9: Make Predictions on New Data\n**Task:** Use your trained model to predict the sentiment of the following review:\n\n> _\"The movie was absolutely fantastic and kept me hooked till the end.\"_\n\n**Question:** What challenges might arise when predicting on new data?","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 10: Wrap-up\n1. How well did your model perform?\n2. What challenges did you face while implementing it from scratch?\n3. What improvements would you suggest for the future?","metadata":{}},{"cell_type":"markdown","source":"### Notes (if any):","metadata":{}}]}